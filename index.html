<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Hancheng Min </title> <meta name="author" content="Hancheng Min"> <meta name="description" content="Hancheng Min's personal website "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a7b9346409f983912ffd90ed5bf7aca8"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/sjtu_logo.png?fc063755f105dfb794697826af70307c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="hanchmin.github.io/"> <script src="/assets/js/theme.js?1bdf01d54938613f3a7fc47c8da5491e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/updates/">Updates </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span style="color: #990000 ;font-weight:bold">Hancheng</span> Min </h1> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile-1400.webp"></source> <img src="/assets/img/profile.png" class="img-fluid z-depth-1 rounded-circle" width="auto" height="auto" max-width="10rem" alt="profile.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <div class="author-details"> <p> <strong>Tenure-track Associate Professor</strong><br> <em>Institute of Natural Sciences &amp; School of Mathematical Sciences</em><br> <em>Shanghai Jiao Tong University</em><br> </p> <p class="meta-links"> <a href="/assets/pdf/cv_hm.pdf" target="_blank">[CV]</a> / <a href="https://scholar.google.com/citations?user=XgQjPZIAAAAJ&amp;hl=en" target="_blank" rel="external nofollow noopener">[Google Scholar]</a> / <strong>Email: </strong><em>hanchmin [at] sjtu [dot] edu [dot] cn</em> </p> </div> <div id="bio-content"> <p>I am a Tenure-track Associate Professor at the <a href="https://ins.sjtu.edu.cn/" rel="external nofollow noopener" target="_blank">Institute of Natural Sciences (INS)</a> and the <a href="https://math.sjtu.edu.cn" rel="external nofollow noopener" target="_blank">School of Mathematics (SMS)</a>, <a href="https://www.sjtu.edu.cn/" rel="external nofollow noopener" target="_blank">Shanghai Jiao Tong Univeristy</a>. My research centers around building mathematical principles that facilitates the interplay between machine learning and dynamical systems. Recently, I am mainly interested in analyzing gradient-based optimization algorithms on overparametrized neural networks from a dynamical system perspective.</p> <span class="toggle-indicator-container"> <button id="toggle-bio-button" class="btn btn-link toggle-indicator"> More <i class="fas fa-chevron-down"></i> </button> </span> <div id="more-bio" style="display: none;"> <p>Prior to Join SJTU, I was a Postdoc Researcher at <a href="https://ideas.seas.upenn.edu/" rel="external nofollow noopener" target="_blank">Center for Innovation in Data Engineering and Science (IDEAS)</a>, University of Pennsylvania, advised by <a href="http://vision.jhu.edu/rvidal.html" rel="external nofollow noopener" target="_blank">Prof. René Vidal</a> from 2023 to 2025. I received Ph.D. degree from Johns Hopkins University, where I am fortunate to be advised by <a href="http://mallada.ece.jhu.edu" rel="external nofollow noopener" target="_blank">Prof. Enrique Mallada</a> and <a href="http://vision.jhu.edu/rvidal.html" rel="external nofollow noopener" target="_blank">Prof. René Vidal</a>. Before Hopkins, I received Master’s degree in Systems Engineering from University of Pennsylvannia and Bachelor’s degree in Automation from Tongji Univerisity, Shanghai.</p> </div> </div> </div> <div class="content-section card-style"> <h2>Recent Updates</h2> <div class="news"> <div class="news-list"> <div class="news-item" style="margin-bottom: 10px;"> [<strong>Feb, 11, 2026</strong>] Our tutorial paper <i>On the Convergence, Implicit Bias and Edge of Stability of Gradient Descent in Deep Learning</i> has been accepted to <strong>IEEE Signal Processing Magazine</strong> ! </div> <div class="news-item" style="margin-bottom: 10px;"> [<strong>Dec, 10, 2025</strong>] I gave a talk <i>Understanding Incremental Learning with Closed-form Solution to Gradient Flow on Overparamerterized Matrix Factorization</i> at CDC 2025 at Rio </div> <div class="news-item" style="margin-bottom: 10px;"> [<strong>Nov, 23, 2025</strong>] I gave a talk <i>Learning Dynamics in the Feature Learning Regime: Implicit Bias, Neural Collapse, and Robustness</i> at NYU, Shanghai </div> <div class="news-item" style="margin-bottom: 10px;"> [<strong>Sep, 18, 2025</strong>] Our papers <i>Neural Collapse under Gradient Flow on Shallow ReLU Networks for Orthogonally Separable Data</i> and <i>Convergence Rates for Gradient Descent on the Edge of Stability for Overparametrised Least Squares</i> have been accepted to <strong>NeurIPS 2025</strong> ! </div> <div class="news-item" style="margin-bottom: 10px;"> [<strong>Sep, 09, 2025</strong>] I will serve as an Area Chair for ICLR 2026 and AISTATS 2026 </div> </div> <div style="text-align: left;"> <a href="/updates/" style="color: inherit;">and more...</a> </div> </div> </div> <div class="content-section card-style"> <h2>Recent Publications</h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="mmv26spm" class="col-sm-10"> <div class="title">On the Convergence, Implicit Bias and Edge of Stability of Gradient Descent in Deep Learning</div> <div class="author"> <em>H. Min</em>, <a href="https://lemacdonald.github.io/" rel="external nofollow noopener" target="_blank">L. MacDonald</a>, and <a href="http://vision.jhu.edu/rvidal.html" rel="external nofollow noopener" target="_blank">R. Vidal</a> </div> <div class="periodical"> <span style="font-style: italic;">IEEE Signal Processing Magazine (IEEE SPM)</span>, Mar 2026 <span class="links" style="display: inline-block; margin-left: 10px;"> </span> </div> <div class="periodical"> To appear </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/nc-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/nc-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/nc-1400.webp"></source> <img src="/assets/img/publication_preview/nc.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="nc.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mzv2025neurips" class="col-sm-10"> <div class="title">Neural Collapse under Gradient Flow on Shallow ReLU Networks for Orthogonally Separable Data</div> <div class="author"> <em>H. Min</em>, <a href="https://zhihuizhu.github.io/index.html" rel="external nofollow noopener" target="_blank">Z. Zhu</a>, and <a href="http://vision.jhu.edu/rvidal.html" rel="external nofollow noopener" target="_blank">R. Vidal</a> </div> <div class="periodical"> <span style="font-style: italic;">Conference on Neural Information Processing Systems (NeurIPS)</span>, Mar 2025 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a href="http://arxiv.org/abs/2510.21078" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> <a href="/assets/pdf/MZV2025NeurIPS.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/posters/MZV2025NeurIPS.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </span> <div class="abstract hidden"> <p>Among many mysteries behind the success of deep networks lies the exceptional discriminative power of their learned representations as manifested by the intriguing Neural Collapse (NC) phenomenon, where simple feature structures emerge in the last layer of a trained neural network. Prior work on understanding NC theoretically has focused on analyzing the optimization landscape of matrix-factorization-like problems by considering the last-layer features as unconstrained free optimization variables and showing that their global minima exhibit NC. In this paper, we show that gradient flow on a two-layer ReLU network for classifying orthogonally separable data provably exhibits NC, thereby advancing prior results in two ways: First, we relax the assumption of unconstrained features, showing the effect of data structure and nonlinear activations on NC characterizations. Second, we reveal the role of the implicit bias of the training dynamics in facilitating the emergence of NC.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mzv2025neurips</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Neural Collapse under Gradient Flow on Shallow ReLU Networks for Orthogonally Separable Data}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Min, Hancheng and Zhu, Zhihui and Vidal, Ren\'e}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Neural Information Processing Systems (NeurIPS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">recent</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">author+an</span> <span class="p">=</span> <span class="s">{1=highlight}</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/eos-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/eos-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/eos-1400.webp"></source> <img src="/assets/img/publication_preview/eos.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="eos.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mpxmtv2025neurips" class="col-sm-10"> <div class="title">Convergence Rates for Gradient Descent on the Edge of Stability for Overparametrised Least Squares</div> <div class="author"> <a href="https://lemacdonald.github.io/" rel="external nofollow noopener" target="_blank">L. MacDonald</a>, L. Palma, Z. Xu, <em>H. Min</em>, S. Tarmoun, and <a href="http://vision.jhu.edu/rvidal.html" rel="external nofollow noopener" target="_blank">R. Vidal</a> </div> <div class="periodical"> <span style="font-style: italic;">Conference on Neural Information Processing Systems (NeurIPS)</span>, Mar 2025 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a href="http://arxiv.org/abs/2510.17506" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> </span> <div class="abstract hidden"> <p>Classical optimisation theory guarantees monotonic objective decrease for gradient descent (GD) when employed in a small step size, or "stable", regime. In contrast, gradient descent on neural networks is frequently performed in a large step size regime called the "edge of stability", in which the objective decreases non-monotonically with an observed implicit bias towards flat minima. In this paper, we take a step toward quantifying this phenomenon by providing convergence rates for gradient descent with large learning rates in an overparametrised least squares setting. The key insight behind our analysis is that, as a consequence of overparametrisation, the set of global minimisers forms a Riemannian manifold M, which enables the decomposition of the GD dynamics into components parallel and orthogonal to M. The parallel component corresponds to Riemannian gradient descent on the objective sharpness, while the orthogonal component corresponds to a quadratic dynamical system. This insight allows us to derive rates in three regimes characterised by the learning rate size: the subcritical regime, in which transient instability is overcome in finite time before linear convergence to a suboptimally flat global minimum; the critical regime, in which instability persists for all time with a power-law convergence toward the optimally flat global minimum; the supercritical regime, in which instability persists for all time with linear convergence to an oscillation of period two centred on the optimally flat global minimum.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mpxmtv2025neurips</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Convergence Rates for Gradient Descent on the Edge of Stability for Overparametrised Least Squares}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{MacDonald, Lachlan E. and Palma, Leandro and Xu, Ziqing and Min, Hancheng and Tarmoun, Salma and Vidal, Ren\'e}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Neural Information Processing Systems (NeurIPS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">recent</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">author+an</span> <span class="p">=</span> <span class="s">{4=highlight}</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/robust_gmm-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/robust_gmm-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/robust_gmm-1400.webp"></source> <img src="/assets/img/publication_preview/robust_gmm.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="robust_gmm.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mv2025icml" class="col-sm-10"> <div class="title">Gradient Flow Provably Learns Robust Classifiers for Orthonormal GMMs</div> <div class="author"> <em>H. Min</em>, and <a href="http://vision.jhu.edu/rvidal.html" rel="external nofollow noopener" target="_blank">R. Vidal</a> </div> <div class="periodical"> <span style="font-style: italic;">International Conference on Machine Learning (ICML)</span>, Mar 2025 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> <a href="/assets/pdf/MV2025ICML.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/posters/MV2025ICML.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </span> <div class="abstract hidden"> <p>Deep learning-based classifiers are known to be vulnerable to adversarial attacks. Existing methods for defending against such attacks require adding a defense mechanism or modifying the learning procedure (e.g., by adding adversarial examples). This paper shows that for certain data distributions one can learn a provably robust classifier using standard learning methods and without adding a defense mechanism. More specifically, this paper addresses the problem of finding a robust classifier for a binary classification problem in which the data comes from an isotropic mixture of Gaussians with orthonormal cluster centers. First, we characterize the largest \ell_2-attack any classifier can defend against while maintaining high accuracy, and show the existence of optimal robust classifiers achieving this maximum \ell_2-robustness. Next, we show that given data sampled from the orthonormal Gaussian mixture model, gradient flow on a two-layer network with a polynomial ReLU activation and without adversarial examples provably finds an optimal robust classifier.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mv2025icml</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Gradient Flow Provably Learns Robust Classifiers for Orthonormal GMMs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Min, Hancheng and Vidal, Ren\'e}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning ({ICML})}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--8}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">recent</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> </ol> </div> </div> <div class="content-section card-style"> <h2><a href="/publications/" style="color: inherit;">Selected Publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/nc-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/nc-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/nc-1400.webp"></source> <img src="/assets/img/publication_preview/nc.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="nc.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mzv2025neurips" class="col-sm-10"> <div class="title">Neural Collapse under Gradient Flow on Shallow ReLU Networks for Orthogonally Separable Data</div> <div class="author"> <em>H. Min</em>, <a href="https://zhihuizhu.github.io/index.html" rel="external nofollow noopener" target="_blank">Z. Zhu</a>, and <a href="http://vision.jhu.edu/rvidal.html" rel="external nofollow noopener" target="_blank">R. Vidal</a> </div> <div class="periodical"> <span style="font-style: italic;">Conference on Neural Information Processing Systems (NeurIPS)</span>, 2025 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a href="http://arxiv.org/abs/2510.21078" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> <a href="/assets/pdf/MZV2025NeurIPS.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/posters/MZV2025NeurIPS.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </span> <div class="abstract hidden"> <p>Among many mysteries behind the success of deep networks lies the exceptional discriminative power of their learned representations as manifested by the intriguing Neural Collapse (NC) phenomenon, where simple feature structures emerge in the last layer of a trained neural network. Prior work on understanding NC theoretically has focused on analyzing the optimization landscape of matrix-factorization-like problems by considering the last-layer features as unconstrained free optimization variables and showing that their global minima exhibit NC. In this paper, we show that gradient flow on a two-layer ReLU network for classifying orthogonally separable data provably exhibits NC, thereby advancing prior results in two ways: First, we relax the assumption of unconstrained features, showing the effect of data structure and nonlinear activations on NC characterizations. Second, we reveal the role of the implicit bias of the training dynamics in facilitating the emergence of NC.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mzv2025neurips</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Neural Collapse under Gradient Flow on Shallow ReLU Networks for Orthogonally Separable Data}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Min, Hancheng and Zhu, Zhihui and Vidal, Ren\'e}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Neural Information Processing Systems (NeurIPS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">recent</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">author+an</span> <span class="p">=</span> <span class="s">{1=highlight}</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/robust_gmm-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/robust_gmm-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/robust_gmm-1400.webp"></source> <img src="/assets/img/publication_preview/robust_gmm.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="robust_gmm.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mv2025icml" class="col-sm-10"> <div class="title">Gradient Flow Provably Learns Robust Classifiers for Orthonormal GMMs</div> <div class="author"> <em>H. Min</em>, and <a href="http://vision.jhu.edu/rvidal.html" rel="external nofollow noopener" target="_blank">R. Vidal</a> </div> <div class="periodical"> <span style="font-style: italic;">International Conference on Machine Learning (ICML)</span>, 2025 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> <a href="/assets/pdf/MV2025ICML.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/posters/MV2025ICML.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </span> <div class="abstract hidden"> <p>Deep learning-based classifiers are known to be vulnerable to adversarial attacks. Existing methods for defending against such attacks require adding a defense mechanism or modifying the learning procedure (e.g., by adding adversarial examples). This paper shows that for certain data distributions one can learn a provably robust classifier using standard learning methods and without adding a defense mechanism. More specifically, this paper addresses the problem of finding a robust classifier for a binary classification problem in which the data comes from an isotropic mixture of Gaussians with orthonormal cluster centers. First, we characterize the largest \ell_2-attack any classifier can defend against while maintaining high accuracy, and show the existence of optimal robust classifiers achieving this maximum \ell_2-robustness. Next, we show that given data sampled from the orthonormal Gaussian mixture model, gradient flow on a two-layer network with a polynomial ReLU activation and without adversarial examples provably finds an optimal robust classifier.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mv2025icml</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Gradient Flow Provably Learns Robust Classifiers for Orthonormal GMMs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Min, Hancheng and Vidal, Ren\'e}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning ({ICML})}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--8}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">recent</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/coherence.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/coherence.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/coherence.gif-1400.webp"></source> <img src="/assets/img/publication_preview/coherence.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="coherence.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mpm2025aut" class="col-sm-10"> <div class="title">A Frequency Domain Analysis of Slow Coherency in Networked Systems</div> <div class="author"> <em>H. Min</em>, <a href="https://www.richardpates.com/" rel="external nofollow noopener" target="_blank">R. Pates</a>, and <a href="https://mallada.ece.jhu.edu/" rel="external nofollow noopener" target="_blank">E. Mallada</a> </div> <div class="periodical"> <span style="font-style: italic;">Automatica</span>, 2025 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a href="http://arxiv.org/abs/2302.08438" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> </span> <div class="abstract hidden"> <p>Network coherence generally refers to the emergence of simple aggregated dynamical behaviours, despite heterogeneity in the dynamics of the subsystems that constitute the network. In this paper, we develop a general frequency domain framework to analyze and quantify the level of network coherence that a system exhibits by relating coherence with a low-rank property of the system’s input-output response. More precisely, for a networked system with linear dynamics and coupling, we show that, as the network’s \empheffective algebraic connectivity grows, the system transfer matrix converges to a rank-one transfer matrix representing the coherent behavior. Interestingly, the non-zero eigenvalue of such a rank-one matrix is given by the harmonic mean of individual nodal dynamics, and we refer to it as the coherent dynamics. Our analysis unveils the frequency-dependent nature of coherence and a non-trivial interplay between dynamics and network topology. We further show that many networked systems can exhibit similar coherent behavior by establishing a concentration result in a setting with randomly chosen individual nodal dynamics.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">mpm2025aut</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Frequency Domain Analysis of Slow Coherency in Networked Systems}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Min, Hancheng and Pates, Richard and Mallada, Enrique}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Automatica}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{174}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{112184}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/dir_flow.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/dir_flow.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/dir_flow.gif-1400.webp"></source> <img src="/assets/img/publication_preview/dir_flow.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="dir_flow.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mmv2024iclr" class="col-sm-10"> <div class="title">Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization</div> <div class="author"> <em>H. Min</em>, <a href="https://mallada.ece.jhu.edu/" rel="external nofollow noopener" target="_blank">E. Mallada</a>, and <a href="http://vision.jhu.edu/rvidal.html" rel="external nofollow noopener" target="_blank">R. Vidal</a> </div> <div class="periodical"> <span style="font-style: italic;">International Conference on Learning Representations (ICLR)</span>, 2024 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a href="http://arxiv.org/abs/2307.12851" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> <a href="/assets/pdf/MMV2024ICLR.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/posters/MMV2024ICLR.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Poster</a> <a href="/assets/slides/MMV2024ICLR.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Slides</a> </span> <div class="abstract hidden"> <p>This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons’ directional dynamics allows us to provide an \mathcalO(\frac\log n\sqrtμ) upper bound on the time it takes for all neurons to achieve good alignment with the input data, where n is the number of data points and μmeasures how well the data are separated. After the early alignment phase, the loss converges to zero at a \mathcalO(\frac1t) rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mmv2024iclr</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Min, Hancheng and Mallada, Enrique and Vidal, Ren\'e}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations ({ICLR})}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-8}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/lin_conv-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/lin_conv-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/lin_conv-1400.webp"></source> <img src="/assets/img/publication_preview/lin_conv.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="lin_conv.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mvm2023icml" class="col-sm-10"> <div class="title">On the Convergence of Gradient Flow on Multi-layer Linear Models</div> <div class="author"> <em>H. Min</em>, <a href="http://vision.jhu.edu/rvidal.html" rel="external nofollow noopener" target="_blank">R. Vidal</a>, and <a href="https://mallada.ece.jhu.edu/" rel="external nofollow noopener" target="_blank">E. Mallada</a> </div> <div class="periodical"> <span style="font-style: italic;">International Conference on Machine Learning (ICML)</span>, 2023 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> <a href="/assets/pdf/MVM2023ICML.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/posters/MVM2023ICML.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Poster</a> <a href="/assets/slides/MVM2023ICML.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Slides</a> </span> <div class="abstract hidden"> <p>In this paper, we analyze the convergence of gradient flow on a multi-layer linear model with a loss function of the form f(W_1W_2⋯W_L). We show that when f satisfies the gradient dominance property, proper weight initialization leads to exponential convergence of the gradient flow to a global minimum of the loss. Moreover, the convergence rate depends on two trajectory-specific quantities that are controlled by the weight initialization: the imbalance matrices, which measure the difference between the weights of adjacent layers, and the least singular value of the weight product W=W_1W_2⋯W_L. Our analysis exploits the fact that the gradient of the overparameterized loss can be written as the composition of the non-overparametrized gradient with a time-varying (weight-dependent) linear operator whose smallest eigenvalue controls the convergence rate. The key challenge we address is to derive a uniform lower bound for this time-varying eigenvalue that lead to improved rates for several multi-layer network models studied in the literature.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mvm2023icml</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On the Convergence of Gradient Flow on Multi-layer Linear Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Min, Hancheng and Vidal, Ren\'e and Mallada, Enrique}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning ({ICML})}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{24850--24887}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{202}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> </ol> </div> </div> </article> </div> <script>document.addEventListener("DOMContentLoaded",function(){const e=document.getElementById("toggle-bio-button"),n=document.getElementById("more-bio"),t=document.querySelector(".toggle-indicator-container"),o=document.getElementById("bio-content");if(e&&n&&t&&o)if(""!==n.innerHTML.trim()){const i=o.querySelector("p");i?(i.appendChild(t),e.addEventListener("click",function(){"none"===n.style.display||""===n.style.display?(n.style.display="block",e.innerHTML='Less <i class="fas fa-chevron-up"></i>'):(n.style.display="none",e.innerHTML='More <i class="fas fa-chevron-down"></i>')}),t.style.display="inline-block"):t.style.display="none"}else t.style.display="none"});</script> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Hancheng Min. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: February 11, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>