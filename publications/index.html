<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Hancheng Min </title> <meta name="author" content="Hancheng Min"> <meta name="description" content="List of my publications and preprints"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/university_shield_small_blue.png?5eea9293dfd5ff7d7789a5d23441bc88"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="hanchmin.github.io/publications/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Hancheng</span> Min </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item dropdown active"> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Research <span class="sr-only">(current)</span> </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item active" href="/publications/">Publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">Projects</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/cv_hm.pdf">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/update/">Update </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">List of my publications and preprints</p> </header> <article> <div class="publications"> <h2>Journal and Preprints</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/coherence.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/coherence.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/coherence.gif-1400.webp"></source> <img src="/assets/img/publication_preview/coherence.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="coherence.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="min2023a" class="col-sm-8"> <div class="title">A Frequency Domain Analysis of Slow Coherency in Networked Systems</div> <div class="author"> <em>Hancheng Min</em>, Richard Pates, and Enrique Mallada</div> <div class="periodical"> <span style="font-style: italic;">Automatica</span>, 2023 </div> <div class="periodical"> submitted, under revision </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2302.08438" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/MPM2023Preprint.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Network coherence generally refers to the emergence of simple aggregated dynamical behaviours, despite heterogeneity in the dynamics of the subsystems that constitute the network. In this paper, we develop a general frequency domain framework to analyze and quantify the level of network coherence that a system exhibits by relating coherence with a low-rank property of the system’s input-output response. More precisely, for a networked system with linear dynamics and coupling, we show that, as the network’s \empheffective algebraic connectivity grows, the system transfer matrix converges to a rank-one transfer matrix representing the coherent behavior. Interestingly, the non-zero eigenvalue of such a rank-one matrix is given by the harmonic mean of individual nodal dynamics, and we refer to it as the coherent dynamics. Our analysis unveils the frequency-dependent nature of coherence and a non-trivial interplay between dynamics and network topology. We further show that many networked systems can exhibit similar coherent behavior by establishing a concentration result in a setting with randomly chosen individual nodal dynamics.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/safe_rl-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/safe_rl-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/safe_rl-1400.webp"></source> <img src="/assets/img/publication_preview/safe_rl.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="safe_rl.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="cmbm2021tac" class="col-sm-8"> <div class="title">Learning to Act Safely with Limited Exposure and Almost Sure Certainty</div> <div class="author"> Agustin Castellano, <em>Hancheng Min</em>, Juan Bazerque, and Enrique Mallada</div> <div class="periodical"> <span style="font-style: italic;">IEEE Transaction on Automatic Control</span>, May 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/CMBM2023TAC.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>This paper aims to put forward the concept that learning to take safe actions in unknown environments, even with probability one guarantees, can be achieved without the need for an unbounded number of exploratory trials, provided that one is willing to navigate trade-offs between optimality, level of exposure to unsafe events, and the maximum detection time of unsafe actions. We illustrate this concept in two complementary settings. We first focus on the canonical multi-armed bandit problem and seek to study the intrinsic trade-offs of learning safety in the presence of uncertainty. Under mild assumptions on sufficient exploration, we provide an algorithm that provably detects all unsafe machines in an (expected) finite number of rounds. The analysis also unveils a trade-off between the number of rounds needed to secure the environment and the probability of discarding safe machines. We then consider the problem of finding optimal policies for a Markov Decision Process (MDP) with almost sure constraints. We show that the (action) value function satisfies a barrier-based decomposition which allows for the identification of feasible policies independently of the reward process. Using this decomposition, we develop a Barrier-learning algorithm, that identifies such unsafe state-action pairs in a finite expected number of steps. Our analysis further highlights a trade-off between the time lag for the underlying MDP necessary to detect unsafe actions, and the level of exposure to unsafe events. Simulations corroborate our theoretical findings, further illustrating the aforementioned trade-offs, and suggesting that safety constraints can further speed up the learning process.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/lin_conv_two_layer-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/lin_conv_two_layer-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/lin_conv_two_layer-1400.webp"></source> <img src="/assets/img/publication_preview/lin_conv_two_layer.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="lin_conv_two_layer.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mtvm22prpt" class="col-sm-8"> <div class="title">Convergence and Implicit Bias of Gradient Flow on Overparametrized Linear Networks</div> <div class="author"> <em>Hancheng Min</em>, Salma Tarmoun, René Vidal, and Enrique Mallada</div> <div class="periodical"> <span style="font-style: italic;">Journal of Machine Learning Research</span>, May 2023 </div> <div class="periodical"> in preparation </div> <div class="links"> <a href="http://arxiv.org/abs/2105.06351" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/MTVM2022Preprint.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/aggr_model-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/aggr_model-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/aggr_model-1400.webp"></source> <img src="/assets/img/publication_preview/aggr_model.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="aggr_model.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mpm2021lcss" class="col-sm-8"> <div class="title">Accurate Reduced Order Models for Coherent Heterogeneous Generators</div> <div class="author"> <em>Hancheng Min</em>, Fernando Paganini, and Enrique Mallada</div> <div class="periodical"> <span style="font-style: italic;">IEEE Control Systems Letters (L-CSS)</span>, Nov 2021 </div> <div class="periodical"> also in ACC 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1909.12864" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/MPM2021LCSS.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We introduce a novel framework to approximate the aggregate frequency dynamics of coherent synchronous generators. By leveraging recent results on dynamics concentration of tightly connected networks, we develop a hierarchy of reduced order models –based on frequency weighted balanced truncation– that accurately approximate the aggregate system response. Our results outperform existing aggregation techniques and can be shown to monotonically improve the approximation as the hierarchy order increases.</p> </div> </div> </div> </li> </ol> <h2>Conference</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="mv2024icml" class="col-sm-8"> <div class="title">Can Implicit Bias Imply Adversarial Robustness?</div> <div class="author"> <em>Hancheng Min</em>, and Rene Vidal</div> <div class="periodical"> <span style="font-style: italic;">In Proceedings of the 41th International Conference on Machine Learning (ICML)</span>, Jul 2024 </div> <div class="periodical"> to appear </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>The implicit bias of gradient-based training algorithms has been considered mostly beneficial as it leads to trained networks that often generalize well. However, \citetfrei2023the show that such implicit bias can harm adversarial robustness. Specifically, when the data consists of clusters with small inter-cluster correlation, a shallow (two-layer) ReLU network trained by gradient flow generalizes well, but it is not robust to adversarial attacks of small radius, despite the existence of a much more robust classifier that can be explicitly constructed from a shallow network. In this paper, we extend recent analyses of neuron alignment to show that a shallow network with a polynomial ReLU activation (pReLU) trained by gradient flow not only generalizes well but is also robust to adversarial attacks. Our results highlight the importance of the interplay between data structure and architecture design in the implicit bias and robustness of trained networks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/freq_osc-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/freq_osc-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/freq_osc-1400.webp"></source> <img src="/assets/img/publication_preview/freq_osc.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="freq_osc.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jmz24pscc" class="col-sm-8"> <div class="title">Oscillations-Aware Frequency Security Assessment via Efficient Worst-Case Frequency Nadir Computation</div> <div class="author"> Yan Jiang, <em>Hancheng Min</em>, and Baosen Zhang</div> <div class="periodical"> <span style="font-style: italic;">In Power Systems Computation Conference</span>, Jun 2024 </div> <div class="periodical"> to appear </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/JMZ2023Preprint.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Frequency security assessment following major disturbances has long been one of the central tasks in power system operations. The standard approach is to study the center of inertia frequency, an aggregate signal for an entire system, to avoid analyzing the frequency signal at individual buses. However, as the amount of low-inertia renewable resources in a grid increases, the center of inertia frequency is becoming too coarse to provide reliable frequency security assessment. In this paper, we propose an efficient algorithm to determine the worst-case frequency nadir across all buses for bounded power disturbances, as well as identify the power disturbances leading to that severest scenario. The proposed algorithm allows oscillation-aware frequency security assessment without conducting exhaustive simulations and intractable analysis.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/dir_flow.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/dir_flow.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/dir_flow.gif-1400.webp"></source> <img src="/assets/img/publication_preview/dir_flow.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="dir_flow.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mmv2024iclr" class="col-sm-8"> <div class="title">Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization</div> <div class="author"> <em>Hancheng Min</em>, Enrique Mallada, and René Vidal</div> <div class="periodical"> <span style="font-style: italic;">In International Conference on Learning Representations (ICLR)</span>, May 2024 </div> <div class="periodical"> to appear </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2307.12851" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/MMV2024ICLR.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons’ directional dynamics allows us to provide an \mathcalO(\frac\log n\sqrtμ) upper bound on the time it takes for all neurons to achieve good alignment with the input data, where n is the number of data points and μmeasures how well the data are separated. After the early alignment phase, the loss converges to zero at a \mathcalO(\frac1t) rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/lin_conv-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/lin_conv-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/lin_conv-1400.webp"></source> <img src="/assets/img/publication_preview/lin_conv.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="lin_conv.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mvm2023icml" class="col-sm-8"> <div class="title">On the Convergence of Gradient Flow on Multi-layer Linear Models</div> <div class="author"> <em>Hancheng Min</em>, René Vidal, and Enrique Mallada</div> <div class="periodical"> <span style="font-style: italic;">In The 40th International Conference on Machine Learning (ICML)</span>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/MVM2023ICML.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>In this paper, we analyze the convergence of gradient flow on a multi-layer linear model with a loss function of the form f(W_1W_2⋯W_L). We show that when f satisfies the gradient dominance property, proper weight initialization leads to exponential convergence of the gradient flow to a global minimum of the loss. Moreover, the convergence rate depends on two trajectory-specific quantities that are controlled by the weight initialization: the imbalance matrices, which measure the difference between the weights of adjacent layers, and the least singular value of the weight product W=W_1W_2⋯W_L. Our analysis exploits the fact that the gradient of the overparameterized loss can be written as the composition of the non-overparametrized gradient with a time-varying (weight-dependent) linear operator whose smallest eigenvalue controls the convergence rate. The key challenge we address is to derive a uniform lower bound for this time-varying eigenvalue that lead to improved rates for several multi-layer network models studied in the literature.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/loss_100-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/loss_100-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/loss_100-1400.webp"></source> <img src="/assets/img/publication_preview/loss_100.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="loss_100.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="xu2023aistat" class="col-sm-8"> <div class="title">Linear Convergence of Gradient Descent For Finite Width Over-parametrized Linear Networks With General Initialization</div> <div class="author"> Ziqing Xu, <em>Hancheng Min</em>, Salma Tarmoun, Enrique Mallada, and Rene Vidal</div> <div class="periodical"> <span style="font-style: italic;">In Proceedings of The 26th International Conference on Artificial Intelligence and Statistics</span>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/XMTMV2023AISTATS.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Recent theoretical analyses of the convergence of gradient descent (GD) to a global minimum for over-parametrized neural networks make strong assumptions on the step size (infinitesimal), the hidden-layer width (infinite), or the initialization (spectral, balanced). In this work, we relax these assumptions and derive a linear convergence rate for two-layer linear networks trained using GD on the squared loss in the case of finite step size, finite width and general initialization. Despite the generality of our analysis, our rate estimates are significantly tighter than those of prior work. Moreover, we provide a time-varying step size rule that monotonically improves the convergence rate as the loss function decreases to zero. Numerical experiments validate our findings.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/inter_area.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/inter_area.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/inter_area.gif-1400.webp"></source> <img src="/assets/img/publication_preview/inter_area.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="inter_area.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mm2023l4dc" class="col-sm-8"> <div class="title">Learning Coherent Clusters in Weakly-Connected Network Systems</div> <div class="author"> <em>Hancheng Min</em>, and Enrique Mallada</div> <div class="periodical"> <span style="font-style: italic;">In Proceedings of The 5th Annual Learning for Dynamics and Control Conference</span>, Jun 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2211.15301" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/MM2023L4DC.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We propose a structure-preserving model-reduction methodology for large-scale dynamic networks with tightly-connected components. First, the coherent groups are identified by a spectral clustering algorithm on the graph Laplacian matrix that models the network feedback. Then, a reduced network is built, where each node represents the aggregate dynamics of each coherent group, and the reduced network captures the dynamic coupling between the groups. We provide an upper bound on the approximation error when the network graph is randomly generated from a weight stochastic block model. Finally, numerical experiments align with and validate our theoretical findings.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/net_red_two-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/net_red_two-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/net_red_two-1400.webp"></source> <img src="/assets/img/publication_preview/net_red_two.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="net_red_two.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mm2023acc" class="col-sm-8"> <div class="title">Spectral clustering and model reduction for weakly-connected coherent network systems</div> <div class="author"> <em>Hancheng Min</em>, and Enrique Mallada</div> <div class="periodical"> <span style="font-style: italic;">In 2023 American Control Conference (ACC)</span>, Jun 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2209.13701" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/MM2023ACC.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>We propose a novel model-reduction methodology for large-scale dynamic networks with tightly-connected components. First, the coherent groups are identified by a spectral clustering algorithm on the graph Laplacian matrix that models the network feedback. Then, a reduced network is built, where each node represents the aggregate dynamics of each coherent group, and the reduced network captures the dynamic coupling between the groups. Our approach is theoretically justified under a random graph setting. Finally, numerical experiments align with and validate our theoretical findings.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/safe_rl_l4dc-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/safe_rl_l4dc-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/safe_rl_l4dc-1400.webp"></source> <img src="/assets/img/publication_preview/safe_rl_l4dc.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="safe_rl_l4dc.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="cmbm2022l4dc" class="col-sm-8"> <div class="title">Reinforcement Learning with Almost Sure Constraints</div> <div class="author"> Agustin Castellano, <em>Hancheng Min</em>, Juan Andrés Bazerque, and Enrique Mallada</div> <div class="periodical"> <span style="font-style: italic;">In The 4th Annual Learning for Dynamics and Control Conference</span>, Jun 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/CMBM2022L4DC.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>In this work we address the problem of finding feasible policies for Constrained Markov Decision Processes under probability one constraints. We argue that stationary policies are not sufficient for solving this problem, and that a rich class of policies can be found by endowing the controller with a scalar quantity, so called budget, that tracks how close the agent is to violating the constraint. We show that the minimal budget required to act safely can be obtained as the smallest fixed point of a Bellman-like operator, for which we analyze its convergence properties. We also show how to learn this quantity when the true kernel of the Markov decision process is not known, while providing sample-complexity bounds. The utility of knowing this minimal budget relies in that it can aid in the search of optimal or near-optimal policies by shrinking down the region of the state space the agent must navigate. Simulations illustrate the different nature of probability one constraints against the typically used constraints in expectation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/lin_conv_two_layer-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/lin_conv_two_layer-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/lin_conv_two_layer-1400.webp"></source> <img src="/assets/img/publication_preview/lin_conv_two_layer.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="lin_conv_two_layer.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mtvm2021icml" class="col-sm-8"> <div class="title">On the Explicit Role of Initialization on the Convergence and Implicit Bias of Overparametrized Linear Networks</div> <div class="author"> <em>Hancheng Min</em>, Salma Tarmoun, René Vidal, and Enrique Mallada</div> <div class="periodical"> <span style="font-style: italic;">In The 38th International Conference on Machine Learning (ICML)</span>, Jul 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/MTVM2021ICML.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Neural networks trained via gradient descent with random initialization and without any regularization enjoy good generalization performance in practice despite being highly overparametrized. A promising direction to explain this phenomenon is to study how initialization and overparametrization affect convergence and implicit bias of training algorithms. In this paper, we present a novel analysis of single-hidden-layer linear networks trained under gradient flow, which connects initialization, optimization, and overparametrization. Firstly, we show that the squared loss converges exponentially to its optimum at a rate that depends on the level of imbalance of the initialization. Secondly, we show that proper initialization constrains the dynamics of the network parameters to lie within an invariant set. In turn, minimizing the loss over this set leads to the min-norm solution. Finally, we show that large hidden layer width, together with (properly scaled) random initialization, ensures proximity to such an invariant set during training, allowing us to derive a novel non-asymptotic upper-bound on the distance between the trained network and the min-norm solution.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/iceland_step_coherence-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/iceland_step_coherence-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/iceland_step_coherence-1400.webp"></source> <img src="/assets/img/publication_preview/iceland_step_coherence.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="iceland_step_coherence.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mm2019cdc" class="col-sm-8"> <div class="title">Dynamics Concentration of Tightly-Connected Large-Scale Networks</div> <div class="author"> <em>Hancheng Min</em>, and Enrique Mallada</div> <div class="periodical"> <span style="font-style: italic;">In 58th IEEE Conference on Decision and Control (CDC)</span>, Dec 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1903.06017" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/MM2019CDC.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>The ability to achieve coordinated behavior –engineered or emergent– on networked systems has attracted widespread interest over several fields. This has led to remarkable advances on the development of a theoretical understanding of the conditions under which agents within a network can reach agreement (consensus) or develop coordinated behaviors such as synchronization. However, fewer advances have been made toward explaining another commonly observed phenomena in tightly-connected networks systems: output responses of nodes in the networks are almost identical to each other despite heterogeneity in their individual dynamics. In this paper, we leverage tools from high-dimensional probability to provide an initial answer to this phenomena. More precisely, we show that for linear networks of nodal random transfer functions, as the networks size and connectivity grows, every node in the network follows the same response to an input or disturbance – irrespectively of the source of this input. We term this behavior as dynamics concentration as it stems from the fact that the network transfer matrix uniformly converges in probability to a unique dynamic response –i.e., it concentrates– determined by the distribution of the random transfer function of each node. We further discuss the implications of our analysis in the context of model reduction and robustness, and provide numerical evidence that similar phenomena occur in small deterministic networks over a properly defined frequency band.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="amk2018icra" class="col-sm-8"> <div class="title">Voronoi-Based Coverage Control of Pan/Tilt/Zoom Camera Networks</div> <div class="author"> O. Arslan, H. Min, and D. E. Koditschek</div> <div class="periodical"> <span style="font-style: italic;">In 2018 IEEE International Conference on Robotics and Automation (ICRA)</span>, May 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="https://repository.upenn.edu/cgi/viewcontent.cgi?article=1907&amp;context=ese_papers" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> </div> </div> </div> </li> </ol> <h2>Thesis</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/cov_ctrl-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/cov_ctrl-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/cov_ctrl-1400.webp"></source> <img src="/assets/img/publication_preview/cov_ctrl.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="cov_ctrl.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="min_MScThesis2018" class="col-sm-8"> <div class="title">On Balancing Event and Area Coverage in Mobile Sensor Networks</div> <div class="author"> <em>Hancheng Min</em> </div> <div class="periodical"> <span style="font-style: italic;">University of Pennsylvania</span>, 2018 </div> <div class="periodical"> Master’s Thesis </div> <div class="links"> <a href="https://repository.upenn.edu/cgi/viewcontent.cgi?article=1910&amp;context=ese_papers" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> </div> </div> </div> </li></ol> <h2>Misc</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="mm2021misc" class="col-sm-8"> <div class="title">Coherence and Concentration in Tightly-Connected Networks</div> <div class="author"> <em>Hancheng Min</em>, Richard Pates, and Enrique Mallada</div> <div class="periodical"> 2021 </div> <div class="periodical"> technical note </div> <div class="links"> <a href="http://arxiv.org/abs/2101.00981" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/MPM2022TN.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Hancheng Min. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: May 02, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>