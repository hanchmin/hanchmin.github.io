<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Hancheng Min </title> <meta name="author" content="Hancheng Min"> <meta name="description" content="List of my publications and preprints"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a7b9346409f983912ffd90ed5bf7aca8"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/university_shield_small_blue.png?5eea9293dfd5ff7d7789a5d23441bc88"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="hanchmin.github.io/publications/"> <script src="/assets/js/theme.js?1bdf01d54938613f3a7fc47c8da5491e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Hancheng</span> Min </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/updates/">Updates </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">List of my publications and preprints</p> </header> <article> <div class="publications"> <h2>Journal and Preprints</h2> <ol class="bibliography"> <li> <div class="row"> <div id="xmtmv2025tmlr" class="col-sm-12"> <div class="title">A Local Polyak-łOjasiewicz and Descent Lemma of Gradient Descent for Overparameterized Linear Models</div> <div class="author"> Z. Xu, <em>H. Min</em>, S. Tarmoun, <a href="https://mallada.ece.jhu.edu/" rel="external nofollow noopener" target="_blank">E. Mallada</a>, and <a href="http://vision.jhu.edu/rvidal.html" rel="external nofollow noopener" target="_blank">R. Vidal</a> </div> <div class="periodical"> <span style="font-style: italic;">Transactions on Machine Learning Research (TMLR)</span>, 2025 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a href="http://arxiv.org/abs/2505.11664" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/XMTMV2025TMLR.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </span> <div class="abstract hidden"> <p>Most prior work on the convergence of gradient descent (GD) for overparameterized neural networks relies on strong assumptions on the step size (infinitesimal), the hidden-layer width (infinite), or the initialization (large, spectral, balanced). Recent efforts to relax these assumptions focus on two-layer linear networks trained with the squared loss. In this work, we derive a linear convergence rate for training two-layer linear neural networks with GD for general losses and under relaxed assumptions on the step size, width, and initialization. A key challenge in deriving this result is that classical ingredients for deriving convergence rates for nonconvex problems, such as the Polyak-Łojasiewicz (PL) condition and Descent Lemma, do not hold globally for overparameterized neural networks. Here, we prove that these two conditions hold locally with local constants that depend on the weights. Then, we provide bounds on these local constants, which depend on the initialization of the weights, the current loss, and the global PL and smoothness constants of the non-overparameterized model. Based on these bounds, we derive a linear convergence rate for GD. Our convergence analysis not only improves upon prior results but also suggests a better choice for the step size, as verified through our numerical experiments.</p> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div id="mpm2025aut" class="col-sm-12"> <div class="title">A Frequency Domain Analysis of Slow Coherency in Networked Systems</div> <div class="author"> <em>H. Min</em>, <a href="https://www.richardpates.com/" rel="external nofollow noopener" target="_blank">R. Pates</a>, and <a href="https://mallada.ece.jhu.edu/" rel="external nofollow noopener" target="_blank">E. Mallada</a> </div> <div class="periodical"> <span style="font-style: italic;">Automatica</span>, 2025 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a href="http://arxiv.org/abs/2302.08438" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> </span> <div class="abstract hidden"> <p>Network coherence generally refers to the emergence of simple aggregated dynamical behaviours, despite heterogeneity in the dynamics of the subsystems that constitute the network. In this paper, we develop a general frequency domain framework to analyze and quantify the level of network coherence that a system exhibits by relating coherence with a low-rank property of the system’s input-output response. More precisely, for a networked system with linear dynamics and coupling, we show that, as the network’s \empheffective algebraic connectivity grows, the system transfer matrix converges to a rank-one transfer matrix representing the coherent behavior. Interestingly, the non-zero eigenvalue of such a rank-one matrix is given by the harmonic mean of individual nodal dynamics, and we refer to it as the coherent dynamics. Our analysis unveils the frequency-dependent nature of coherence and a non-trivial interplay between dynamics and network topology. We further show that many networked systems can exhibit similar coherent behavior by establishing a concentration result in a setting with randomly chosen individual nodal dynamics.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">mpm2025aut</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Frequency Domain Analysis of Slow Coherency in Networked Systems}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Min, Hancheng and Pates, Richard and Mallada, Enrique}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Automatica}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{174}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{112184}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div id="jmz2024pscc" class="col-sm-12"> <div class="title">Oscillations-aware Frequency Security Assessment via Efficient Worst-case Frequency Nadir Computation</div> <div class="author"> <a href="https://www.yanjiang.info/" rel="external nofollow noopener" target="_blank">Y. Jiang</a>, <em>H. Min</em>, and <a href="https://zhangbaosen.github.io/" rel="external nofollow noopener" target="_blank">B. Zhang</a> </div> <div class="periodical"> <span style="font-style: italic;">Electric Power Systems Research (EPSR)</span>, 2024 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a href="http://arxiv.org/abs/2402.16765" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> <a href="/assets/pdf/JMZ2024PSCC.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </span> <div class="abstract hidden"> <p>Frequency security assessment following major disturbances has long been one of the central tasks in power system operations. The standard approach is to study the center of inertia frequency, an aggregate signal for an entire system, to avoid analyzing the frequency signal at individual buses. However, as the amount of low-inertia renewable resources in a grid increases, the center of inertia frequency is becoming too coarse to provide reliable frequency security assessment. In this paper, we propose an efficient algorithm to determine the worst-case frequency nadir across all buses for bounded power disturbances, as well as identify the power disturbances leading to that severest scenario. The proposed algorithm allows oscillations-aware frequency security assessment without conducting exhaustive simulations and intractable analysis.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">jmz2024pscc</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Oscillations-aware Frequency Security Assessment via Efficient Worst-case Frequency Nadir Computation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jiang, Yan and Min, Hancheng and Zhang, Baosen}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Electric Power Systems Research ({EPSR})}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{234}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{110656}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div id="cmbm2021tac" class="col-sm-12"> <div class="title">Learning to Act Safely with Limited Exposure and Almost Sure Certainty</div> <div class="author"> <a href="https://mallada.ece.jhu.edu/people/agustin-castellano/" rel="external nofollow noopener" target="_blank">A. Castellano</a>, <em>H. Min</em>, <a href="https://www.engineering.pitt.edu/people/faculty/juan-bazerque-giusto/" rel="external nofollow noopener" target="_blank">J. Bazerque</a>, and <a href="https://mallada.ece.jhu.edu/" rel="external nofollow noopener" target="_blank">E. Mallada</a> </div> <div class="periodical"> <span style="font-style: italic;">IEEE Transactions on Automatic Control (TAC)</span>, 2023 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> <a href="/assets/pdf/CMBM2023TAC.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </span> <div class="abstract hidden"> <p>This paper aims to put forward the concept that learning to take safe actions in unknown environments, even with probability one guarantees, can be achieved without the need for an unbounded number of exploratory trials, provided that one is willing to navigate trade-offs between optimality, level of exposure to unsafe events, and the maximum detection time of unsafe actions. We illustrate this concept in two complementary settings. We first focus on the canonical multi-armed bandit problem and seek to study the intrinsic trade-offs of learning safety in the presence of uncertainty. Under mild assumptions on sufficient exploration, we provide an algorithm that provably detects all unsafe machines in an (expected) finite number of rounds. The analysis also unveils a trade-off between the number of rounds needed to secure the environment and the probability of discarding safe machines. We then consider the problem of finding optimal policies for a Markov Decision Process (MDP) with almost sure constraints. We show that the (action) value function satisfies a barrier-based decomposition which allows for the identification of feasible policies independently of the reward process. Using this decomposition, we develop a Barrier-learning algorithm, that identifies such unsafe state-action pairs in a finite expected number of steps. Our analysis further highlights a trade-off between the time lag for the underlying MDP necessary to detect unsafe actions, and the level of exposure to unsafe events. Simulations corroborate our theoretical findings, further illustrating the aforementioned trade-offs, and suggesting that safety constraints can further speed up the learning process.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">cmbm2021tac</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning to Act Safely with Limited Exposure and Almost Sure Certainty}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Castellano, Agustin and Min, Hancheng and Bazerque, Juan and Mallada, Enrique}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Automatic Control ({TAC})}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{68}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2979-2994}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TAC.2023.3240925}</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div id="mtvm22prpt" class="col-sm-12"> <div class="title">Convergence and Implicit Bias of Gradient Flow on Overparametrized Linear Networks</div> <div class="author"> <em>H. Min</em>, S. Tarmoun, <a href="http://vision.jhu.edu/rvidal.html" rel="external nofollow noopener" target="_blank">R. Vidal</a>, and <a href="https://mallada.ece.jhu.edu/" rel="external nofollow noopener" target="_blank">E. Mallada</a> </div> <div class="periodical"> <span style="font-style: italic;"></span> 2023 <span class="links" style="display: inline-block; margin-left: 10px;"> <a href="http://arxiv.org/abs/2105.06351" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/MTVM2022Preprint.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </span> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div id="mpm2021lcss" class="col-sm-12"> <div class="title">Accurate Reduced Order Models for Coherent Heterogeneous Generators</div> <div class="author"> <em>H. Min</em>, F. Paganini, and <a href="https://mallada.ece.jhu.edu/" rel="external nofollow noopener" target="_blank">E. Mallada</a> </div> <div class="periodical"> <span style="font-style: italic;">IEEE Control Systems Letters (L-CSS)</span>, 2021 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a href="http://arxiv.org/abs/1909.12864" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> <a href="/assets/pdf/MPM2021LCSS.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/slides/MPM2021LCSS.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Slides</a> </span> <div class="abstract hidden"> <p>We introduce a novel framework to approximate the aggregate frequency dynamics of coherent synchronous generators. By leveraging recent results on dynamics concentration of tightly connected networks, we develop a hierarchy of reduced order models –based on frequency weighted balanced truncation– that accurately approximate the aggregate system response. Our results outperform existing aggregation techniques and can be shown to monotonically improve the approximation as the hierarchy order increases.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">mpm2021lcss</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Accurate Reduced Order Models for Coherent Heterogeneous Generators}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Min, Hancheng and Paganini, Fernando and Mallada, Enrique}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Control Systems Letters ({L-CSS})}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1741-1746}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/LCSYS.2020.3043733}</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> </ol> <h2>Conference</h2> <ol class="bibliography"> <li> <div class="row"> <div id="mv2025cdc" class="col-sm-12"> <div class="title">Understanding Incremental Learning with Closed-form Solution to Gradient Flow on Overparamerterized Matrix Factorization</div> <div class="author"> <em>H. Min</em>, and <a href="http://vision.jhu.edu/rvidal.html" rel="external nofollow noopener" target="_blank">R. Vidal</a> </div> <div class="periodical"> <span style="font-style: italic;">IEEE Conference on Decision and Control (CDC)</span>, 2025 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> </span> <div class="abstract hidden"> <p>Many theoretical studies on neural networks attribute their excellent empirical performance to the implicit bias or regularization induced by first-order optimization algorithms when training networks under certain initialization assumptions. One example is the incremental learning phenomenon in gradient flow (GF) on an overparamerterized matrix factorization problem with small initialization: GF learns a target matrix by sequentially learning its singular values in decreasing order of magnitude over time. In this paper, we develop a quantitative understanding of this incremental learning behavior for GF on the symmetric matrix factorization problem, using its closed-form solution obtained by solving a Riccati-like matrix differential equation. We show that incremental learning emerges from some time-scale separation among dynamics corresponding to learning different components in the target matrix. By decreasing the initialization scale, these time-scale separations become more prominent, allowing one to find low-rank approximations of the target matrix. Lastly, we discuss the possible avenues for extending this analysis to asymmetric matrix factorization problems.</p> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div id="tdlmv2025iccv" class="col-sm-12"> <div class="title">Voyaging into Perpetual Dynamic Scenes from a Single View</div> <div class="author"> <a href="https://tianfr.github.io/" rel="external nofollow noopener" target="_blank">F. Tian</a>, <a href="https://tianjiaoding.com/" rel="external nofollow noopener" target="_blank">T. Ding</a>, <a href="https://peterljq.github.io/" rel="external nofollow noopener" target="_blank">J. Luo</a>, <em>H. Min</em>, and <a href="http://vision.jhu.edu/rvidal.html" rel="external nofollow noopener" target="_blank">R. Vidal</a> </div> <div class="periodical"> <span style="font-style: italic;">IEEE International Conference on Computer Vision (ICCV)</span>, 2025 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a href="http://arxiv.org/abs/2507.04183" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> <a href="/assets/pdf/TDLMV2025ICCV.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/tianfr/DynamicVoyager" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://tianfr.github.io/project/DynamicVoyager/index.html" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Website</a> </span> <div class="abstract hidden"> <p>The problem of generating a perpetual dynamic scene from a single view is an important problem with widespread applications in augmented and virtual reality, and robotics. However, since dynamic scenes regularly change over time, a key challenge is to ensure that different generated views be consistent with the underlying 3D motions. Prior work learns such consistency by training on multiple views, but the generated scene regions often interpolate between training views and fail to generate perpetual views. To address this issue, we propose \ourframework, which reformulates dynamic scene generation as a scene outpainting problem with new dynamic content. As 2D outpainting models struggle at generating 3D consistent motions from a single 2D view, we enrich 2D pixels with information from their 3D rays that facilitates learning of 3D motion consistency. More specifically, we first map the single-view video input to a dynamic point cloud using the estimated video depths. We then render a partial video of the point cloud from a novel view and outpaint the missing regions using ray information (e.g., the distance from a ray to the point cloud) to generate 3D consistent motions. Next, we use the outpainted video to update the point cloud, which is used for outpainting the scene from future novel views. Moreover, we can control the generated content with the input text prompt. Experiments show that our model can generate perpetual scenes with consistent motions along fly-through cameras.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tdlmv2025iccv</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Voyaging into Perpetual Dynamic Scenes from a Single View}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tian, Fengrui and Ding, Tianjiao and Luo, Jinqi and Min, Hancheng and Vidal, Ren\'e}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Computer Vision ({ICCV})}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">recent</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div id="mv2025icml" class="col-sm-12"> <div class="title">Gradient Flow Provably Learns Robust Classifiers for Orthonormal GMMs</div> <div class="author"> <em>H. Min</em>, and <a href="http://vision.jhu.edu/rvidal.html" rel="external nofollow noopener" target="_blank">R. Vidal</a> </div> <div class="periodical"> <span style="font-style: italic;">International Conference on Machine Learning (ICML)</span>, 2025 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> <a href="/assets/pdf/MV2025ICML.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/posters/MV2025ICML.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </span> <div class="abstract hidden"> <p>Deep learning-based classifiers are known to be vulnerable to adversarial attacks. Existing methods for defending against such attacks require adding a defense mechanism or modifying the learning procedure (e.g., by adding adversarial examples). This paper shows that for certain data distributions one can learn a provably robust classifier using standard learning methods and without adding a defense mechanism. More specifically, this paper addresses the problem of finding a robust classifier for a binary classification problem in which the data comes from an isotropic mixture of Gaussians with orthonormal cluster centers. First, we characterize the largest \ell_2-attack any classifier can defend against while maintaining high accuracy, and show the existence of optimal robust classifiers achieving this maximum \ell_2-robustness. Next, we show that given data sampled from the orthonormal Gaussian mixture model, gradient flow on a two-layer network with a polynomial ReLU activation and without adversarial examples provably finds an optimal robust classifier.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mv2025icml</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Gradient Flow Provably Learns Robust Classifiers for Orthonormal GMMs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Min, Hancheng and Vidal, Ren\'e}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning ({ICML})}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--8}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">recent</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div id="ldcmcv2025cvpr" class="col-sm-12"> <div class="title">Concept Lancet: Image Editing with Compositional Representation Transplant</div> <div class="author"> <a href="https://peterljq.github.io/" rel="external nofollow noopener" target="_blank">J. Luo</a>, <a href="https://tianjiaoding.com/" rel="external nofollow noopener" target="_blank">T. Ding</a>, <a href="https://ryanchankh.github.io/" rel="external nofollow noopener" target="_blank">K. Chan</a>, <em>H. Min</em>, C. Callison-Burch, and <a href="http://vision.jhu.edu/rvidal.html" rel="external nofollow noopener" target="_blank">R. Vidal</a> </div> <div class="periodical"> <span style="font-style: italic;">IEEE\CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, 2025 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a href="http://arxiv.org/abs/2504.02828" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> <a href="/assets/pdf/LDCMCV2025CVPR.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/peterljq/Concept-Lancet" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://peterljq.github.io/project/colan/" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Website</a> </span> <div class="abstract hidden"> <p>Diffusion models are widely used for image editing tasks. Existing editing methods often design a representation manipulation procedure (e.g., Cat -&gt; Dog, Sketch -&gt; Painting) by curating an edit direction in the text embedding or score space. However, such a procedure faces a key challenge: overestimating the edit strength harms visual consistency while underestimating it fails the editing task. Notably, each source image may require a different editing strength, and it is costly to search for an appropriate strength via trial-and-error. To address this challenge, we propose ConceptLancent (CoLan), a zero-shot plug-and-play framework for principled representation manipulation in diffusion-based image editing. At inference time, we decompose the source input in the latent (text embedding or diffusion score) space as a sparse linear combination of the representations of the collected visual concepts and phrases. This allows us to accurately estimate the presence of concepts in each image, which informs the edit. Based on the editing task (replace, add, or remove), we perform a customized concept transplant process to impose the corresponding editing direction. To sufficiently model the concept space, we curate a conceptual representation dataset, CoLan 150k, which contains diverse descriptions and scenarios of visual concepts and phrases for the latent dictionary. Experiments on multiple diffusion-based image editing baselines show that methods equipped with CoLan achieve state-of-the-art performance in editing effectiveness and consistency preservation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ldcmcv2025cvpr</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Concept Lancet: Image Editing with Compositional Representation Transplant}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Luo, Jinqi and Ding, Tianjiao and Chan, Kwan Ho Ryan and Min, Hancheng and Callison-Burch, Chris and Vidal, Ren\'e}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE\CVF Conference on Computer Vision and Pattern Recognition ({CVPR})}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div id="xmlmtmv2025aistats" class="col-sm-12"> <div class="title">Understanding the Learning Dynamics of LoRA: A Gradient Flow Perspective on Low-Rank Adaptation in Matrix Factorization</div> <div class="author"> Z. Xu, <em>H. Min</em>, <a href="https://peterljq.github.io/" rel="external nofollow noopener" target="_blank">J. Luo</a>, <a href="https://lemacdonald.github.io/" rel="external nofollow noopener" target="_blank">L. MacDonald</a>, S. Tarmoun, <a href="https://mallada.ece.jhu.edu/" rel="external nofollow noopener" target="_blank">E. Mallada</a>, and <a href="http://vision.jhu.edu/rvidal.html" rel="external nofollow noopener" target="_blank">R. Vidal</a> </div> <div class="periodical"> <span style="font-style: italic;">International Conference on Artificial Intelligence and Statistics (AISTATS)</span>, 2025 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a href="http://arxiv.org/abs/2503.06982" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> <a href="/assets/pdf/XMMLTMV2025AISTATS.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </span> <div class="abstract hidden"> <p>Despite Low-Rank Adaptation’s (LoRA) empirical success in fine-tuning pretrained models, there is little theoretical understanding of how first-order methods with carefully crafted initialization adapt models to new tasks. In this work, we take the first step towards bridging this gap by theoretically analyzing the learning dynamics of LoRA for matrix factorization (MF) under gradient flow (GF), emphasizing the crucial role of initialization. For small initialization, we theoretically show that GF converges to a neighborhood of the optimal solution, with smaller initialization leading to lower final error. Our analysis shows that the final error is affected by the misalignment between the singular spaces of the model and the target matrix, and reducing the initialization scale improves alignment. To address this misalignment, we propose a spectral initialization for LoRA in MF and theoretically prove that GF with small spectral initialization can converge to the target matrix with arbitrary precision. Numerical experiments from MF and image classification validate our findings.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xmlmtmv2025aistats</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Understanding the Learning Dynamics of LoRA: A Gradient Flow Perspective on Low-Rank Adaptation in Matrix Factorization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Ziqing and Min, Hancheng and Luo, Jinqi and MacDonald, Lachlan Ewen and Tarmoun, Salma and Mallada, Enrique and Vidal, Ren\'e}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Artificial Intelligence and Statistics ({AISTATS})}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">recent</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div id="mv2024icml" class="col-sm-12"> <div class="title">Can Implicit Bias Imply Adversarial Robustness?</div> <div class="author"> <em>H. Min</em>, and <a href="http://vision.jhu.edu/rvidal.html" rel="external nofollow noopener" target="_blank">R. Vidal</a> </div> <div class="periodical"> <span style="font-style: italic;">International Conference on Machine Learning (ICML)</span>, 2024 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a href="http://arxiv.org/abs/2405.15942" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> <a href="/assets/pdf/MV2024ICML.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/posters/MV2024ICML.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </span> <div class="abstract hidden"> <p>The implicit bias of gradient-based training algorithms has been considered mostly beneficial as it leads to trained networks that often generalize well. However, Frei et al. (2023) show that such implicit bias can harm adversarial robustness. Specifically, they show that if the data consists of clusters with small inter-cluster correlation, a shallow (two-layer) ReLU network trained by gradient flow generalizes well, but it is not robust to adversarial attacks of small radius. Moreover, this phenomenon occurs despite the existence of a much more robust classifier that can be explicitly constructed from a shallow network. In this paper, we extend recent analyses of neuron alignment to show that a shallow network with a polynomial ReLU activation (pReLU) trained by gradient flow not only generalizes well but is also robust to adversarial attacks. Our results highlight the importance of the interplay between data structure and architecture design in the implicit bias and robustness of trained networks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mv2024icml</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Can Implicit Bias Imply Adversarial Robustness?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Min, Hancheng and Vidal, Ren\'e}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning ({ICML})}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{35687--35718}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{235}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div id="mmv2024iclr" class="col-sm-12"> <div class="title">Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization</div> <div class="author"> <em>H. Min</em>, <a href="https://mallada.ece.jhu.edu/" rel="external nofollow noopener" target="_blank">E. Mallada</a>, and <a href="http://vision.jhu.edu/rvidal.html" rel="external nofollow noopener" target="_blank">R. Vidal</a> </div> <div class="periodical"> <span style="font-style: italic;">International Conference on Learning Representations (ICLR)</span>, 2024 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a href="http://arxiv.org/abs/2307.12851" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> <a href="/assets/pdf/MMV2024ICLR.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/posters/MMV2024ICLR.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Poster</a> <a href="/assets/slides/MMV2024ICLR.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Slides</a> </span> <div class="abstract hidden"> <p>This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons’ directional dynamics allows us to provide an \mathcalO(\frac\log n\sqrtμ) upper bound on the time it takes for all neurons to achieve good alignment with the input data, where n is the number of data points and μmeasures how well the data are separated. After the early alignment phase, the loss converges to zero at a \mathcalO(\frac1t) rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mmv2024iclr</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Min, Hancheng and Mallada, Enrique and Vidal, Ren\'e}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations ({ICLR})}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-8}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div id="mvm2023icml" class="col-sm-12"> <div class="title">On the Convergence of Gradient Flow on Multi-layer Linear Models</div> <div class="author"> <em>H. Min</em>, <a href="http://vision.jhu.edu/rvidal.html" rel="external nofollow noopener" target="_blank">R. Vidal</a>, and <a href="https://mallada.ece.jhu.edu/" rel="external nofollow noopener" target="_blank">E. Mallada</a> </div> <div class="periodical"> <span style="font-style: italic;">International Conference on Machine Learning (ICML)</span>, 2023 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> <a href="/assets/pdf/MVM2023ICML.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/posters/MVM2023ICML.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Poster</a> <a href="/assets/slides/MVM2023ICML.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Slides</a> </span> <div class="abstract hidden"> <p>In this paper, we analyze the convergence of gradient flow on a multi-layer linear model with a loss function of the form f(W_1W_2⋯W_L). We show that when f satisfies the gradient dominance property, proper weight initialization leads to exponential convergence of the gradient flow to a global minimum of the loss. Moreover, the convergence rate depends on two trajectory-specific quantities that are controlled by the weight initialization: the imbalance matrices, which measure the difference between the weights of adjacent layers, and the least singular value of the weight product W=W_1W_2⋯W_L. Our analysis exploits the fact that the gradient of the overparameterized loss can be written as the composition of the non-overparametrized gradient with a time-varying (weight-dependent) linear operator whose smallest eigenvalue controls the convergence rate. The key challenge we address is to derive a uniform lower bound for this time-varying eigenvalue that lead to improved rates for several multi-layer network models studied in the literature.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mvm2023icml</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On the Convergence of Gradient Flow on Multi-layer Linear Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Min, Hancheng and Vidal, Ren\'e and Mallada, Enrique}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning ({ICML})}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{24850--24887}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{202}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div id="xu2023aistat" class="col-sm-12"> <div class="title">Linear Convergence of Gradient Descent For Finite Width Over-parametrized Linear Networks With General Initialization</div> <div class="author"> Z. Xu, <em>H. Min</em>, S. Tarmoun, <a href="https://mallada.ece.jhu.edu/" rel="external nofollow noopener" target="_blank">E. Mallada</a>, and <a href="http://vision.jhu.edu/rvidal.html" rel="external nofollow noopener" target="_blank">R. Vidal</a> </div> <div class="periodical"> <span style="font-style: italic;">International Conference on Artificial Intelligence and Statistics (AISTATS)</span>, 2023 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> <a href="/assets/pdf/XMTMV2023AISTATS.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/slides/XMTMV2023AISTATS.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Slides</a> </span> <div class="abstract hidden"> <p>Recent theoretical analyses of the convergence of gradient descent (GD) to a global minimum for over-parametrized neural networks make strong assumptions on the step size (infinitesimal), the hidden-layer width (infinite), or the initialization (spectral, balanced). In this work, we relax these assumptions and derive a linear convergence rate for two-layer linear networks trained using GD on the squared loss in the case of finite step size, finite width and general initialization. Despite the generality of our analysis, our rate estimates are significantly tighter than those of prior work. Moreover, we provide a time-varying step size rule that monotonically improves the convergence rate as the loss function decreases to zero. Numerical experiments validate our findings.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu2023aistat</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Linear Convergence of Gradient Descent For Finite Width Over-parametrized Linear Networks With General Initialization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Ziqing and Min, Hancheng and Tarmoun, Salma and Mallada, Enrique and Vidal, Ren\'e}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Artificial Intelligence and Statistics ({AISTATS})}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2262--2284}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{206}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div id="mm2023l4dc" class="col-sm-12"> <div class="title">Learning Coherent Clusters in Weakly-Connected Network Systems</div> <div class="author"> <em>H. Min</em>, and <a href="https://mallada.ece.jhu.edu/" rel="external nofollow noopener" target="_blank">E. Mallada</a> </div> <div class="periodical"> <span style="font-style: italic;">Learning for Dynamics and Control Conference (L4DC)</span>, 2023 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a href="http://arxiv.org/abs/2211.15301" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> <a href="/assets/pdf/MM2023L4DC.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/posters/MM2023L4DC.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </span> <div class="abstract hidden"> <p>We propose a structure-preserving model-reduction methodology for large-scale dynamic networks with tightly-connected components. First, the coherent groups are identified by a spectral clustering algorithm on the graph Laplacian matrix that models the network feedback. Then, a reduced network is built, where each node represents the aggregate dynamics of each coherent group, and the reduced network captures the dynamic coupling between the groups. We provide an upper bound on the approximation error when the network graph is randomly generated from a weight stochastic block model. Finally, numerical experiments align with and validate our theoretical findings.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mm2023l4dc</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Coherent Clusters in Weakly-Connected Network Systems}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Min, Hancheng and Mallada, Enrique}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Learning for Dynamics and Control Conference ({L4DC})}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1167--1179}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{211}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div id="mm2023acc" class="col-sm-12"> <div class="title">Spectral Clustering and Model Reduction for Weakly-connected Coherent Network Systems</div> <div class="author"> <em>H. Min</em>, and <a href="https://mallada.ece.jhu.edu/" rel="external nofollow noopener" target="_blank">E. Mallada</a> </div> <div class="periodical"> <span style="font-style: italic;">American Control Conference (ACC)</span>, 2023 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a href="http://arxiv.org/abs/2209.13701" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> <a href="/assets/pdf/MM2023ACC.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/slides/MM2023ACC.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Slides</a> </span> <div class="abstract hidden"> <p>We propose a novel model-reduction methodology for large-scale dynamic networks with tightly-connected components. First, the coherent groups are identified by a spectral clustering algorithm on the graph Laplacian matrix that models the network feedback. Then, a reduced network is built, where each node represents the aggregate dynamics of each coherent group, and the reduced network captures the dynamic coupling between the groups. Our approach is theoretically justified under a random graph setting. Finally, numerical experiments align with and validate our theoretical findings.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mm2023acc</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Spectral Clustering and Model Reduction for Weakly-connected Coherent Network Systems}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Min, Hancheng and Mallada, Enrique}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{American Control Conference ({ACC})}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2957-2962}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.23919/ACC55779.2023.10156212}</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div id="cmbm2023acssc" class="col-sm-12"> <div class="title">Learning Safety Critics via a Non-Contractive Binary Bellman Operator</div> <div class="author"> <a href="https://mallada.ece.jhu.edu/people/agustin-castellano/" rel="external nofollow noopener" target="_blank">A. Castellano</a>, <em>H. Min</em>, <a href="https://www.engineering.pitt.edu/people/faculty/juan-bazerque-giusto/" rel="external nofollow noopener" target="_blank">J. Bazerque</a>, and <a href="https://mallada.ece.jhu.edu/" rel="external nofollow noopener" target="_blank">E. Mallada</a> </div> <div class="periodical"> <span style="font-style: italic;">2023 57th Asilomar Conference on Signals, Systems, and Computers (ACSSC)</span>, 2023 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> </span> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">cmbm2023acssc</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Castellano, Agustin and Min, Hancheng and Bazerque, Juan Andrés and Mallada, Enrique}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 57th Asilomar Conference on Signals, Systems, and Computers ({ACSSC})}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning Safety Critics via a Non-Contractive Binary Bellman Operator}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{814-821}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/IEEECONF59524.2023.10476995}</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div id="cmbm2022l4dc" class="col-sm-12"> <div class="title">Reinforcement Learning with Almost Sure Constraints</div> <div class="author"> <a href="https://mallada.ece.jhu.edu/people/agustin-castellano/" rel="external nofollow noopener" target="_blank">A. Castellano</a>, <em>H. Min</em>, <a href="https://www.engineering.pitt.edu/people/faculty/juan-bazerque-giusto/" rel="external nofollow noopener" target="_blank">J. Bazerque</a>, and <a href="https://mallada.ece.jhu.edu/" rel="external nofollow noopener" target="_blank">E. Mallada</a> </div> <div class="periodical"> <span style="font-style: italic;">Learning for Dynamics and Control Conference (L4DC)</span>, 2022 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> <a href="/assets/pdf/CMBM2022L4DC.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </span> <div class="abstract hidden"> <p>In this work we address the problem of finding feasible policies for Constrained Markov Decision Processes under probability one constraints. We argue that stationary policies are not sufficient for solving this problem, and that a rich class of policies can be found by endowing the controller with a scalar quantity, so called budget, that tracks how close the agent is to violating the constraint. We show that the minimal budget required to act safely can be obtained as the smallest fixed point of a Bellman-like operator, for which we analyze its convergence properties. We also show how to learn this quantity when the true kernel of the Markov decision process is not known, while providing sample-complexity bounds. The utility of knowing this minimal budget relies in that it can aid in the search of optimal or near-optimal policies by shrinking down the region of the state space the agent must navigate. Simulations illustrate the different nature of probability one constraints against the typically used constraints in expectation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">cmbm2022l4dc</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Reinforcement Learning with Almost Sure Constraints}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Castellano, Agustin and Min, Hancheng and Bazerque, Juan Andr\'es and Mallada, Enrique}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Learning for Dynamics and Control Conference ({L4DC})}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{559--570}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{168}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div id="mtvm2021icml" class="col-sm-12"> <div class="title">On the Explicit Role of Initialization on the Convergence and Implicit Bias of Overparametrized Linear Networks</div> <div class="author"> <em>H. Min</em>, S. Tarmoun, <a href="http://vision.jhu.edu/rvidal.html" rel="external nofollow noopener" target="_blank">R. Vidal</a>, and <a href="https://mallada.ece.jhu.edu/" rel="external nofollow noopener" target="_blank">E. Mallada</a> </div> <div class="periodical"> <span style="font-style: italic;">International Conference on Machine Learning (ICML)</span>, 2021 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> <a href="/assets/pdf/MTVM2021ICML.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/posters/MTVM2021ICML.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Poster</a> <a href="/assets/slides/MTVM2021ICML.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Slides</a> </span> <div class="abstract hidden"> <p>Neural networks trained via gradient descent with random initialization and without any regularization enjoy good generalization performance in practice despite being highly overparametrized. A promising direction to explain this phenomenon is to study how initialization and overparametrization affect convergence and implicit bias of training algorithms. In this paper, we present a novel analysis of single-hidden-layer linear networks trained under gradient flow, which connects initialization, optimization, and overparametrization. Firstly, we show that the squared loss converges exponentially to its optimum at a rate that depends on the level of imbalance of the initialization. Secondly, we show that proper initialization constrains the dynamics of the network parameters to lie within an invariant set. In turn, minimizing the loss over this set leads to the min-norm solution. Finally, we show that large hidden layer width, together with (properly scaled) random initialization, ensures proximity to such an invariant set during training, allowing us to derive a novel non-asymptotic upper-bound on the distance between the trained network and the min-norm solution.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mtvm2021icml</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On the Explicit Role of Initialization on the Convergence and Implicit Bias of Overparametrized Linear Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Min, Hancheng and Tarmoun, Salma and Vidal, Ren\'e and Mallada, Enrique}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning ({ICML})}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7760--7768}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{139}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div id="mm2019cdc" class="col-sm-12"> <div class="title">Dynamics Concentration of Tightly-Connected Large-Scale Networks</div> <div class="author"> <em>H. Min</em>, and <a href="https://mallada.ece.jhu.edu/" rel="external nofollow noopener" target="_blank">E. Mallada</a> </div> <div class="periodical"> <span style="font-style: italic;">IEEE Conference on Decision and Control (CDC)</span>, 2019 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="abstract btn btn-sm z-depth-1" role="button">Abs</a> <a href="http://arxiv.org/abs/1903.06017" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> <a href="/assets/pdf/MM2019CDC.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/slides/MM2019CDC.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">Slides</a> </span> <div class="abstract hidden"> <p>The ability to achieve coordinated behavior –engineered or emergent– on networked systems has attracted widespread interest over several fields. This has led to remarkable advances on the development of a theoretical understanding of the conditions under which agents within a network can reach agreement (consensus) or develop coordinated behaviors such as synchronization. However, fewer advances have been made toward explaining another commonly observed phenomena in tightly-connected networks systems: output responses of nodes in the networks are almost identical to each other despite heterogeneity in their individual dynamics. In this paper, we leverage tools from high-dimensional probability to provide an initial answer to this phenomena. More precisely, we show that for linear networks of nodal random transfer functions, as the networks size and connectivity grows, every node in the network follows the same response to an input or disturbance – irrespectively of the source of this input. We term this behavior as dynamics concentration as it stems from the fact that the network transfer matrix uniformly converges in probability to a unique dynamic response –i.e., it concentrates– determined by the distribution of the random transfer function of each node. We further discuss the implications of our analysis in the context of model reduction and robustness, and provide numerical evidence that similar phenomena occur in small deterministic networks over a properly defined frequency band.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mm2019cdc</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dynamics Concentration of Tightly-Connected Large-Scale Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Min, Hancheng and Mallada, Enrique}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE Conference on Decision and Control ({CDC})}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{758-763}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/CDC40024.2019.9029796}</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div id="amk2018icra" class="col-sm-12"> <div class="title">Voronoi-Based Coverage Control of Pan/Tilt/Zoom Camera Networks</div> <div class="author"> O. Arslan, H. Min, and D. Koditschek</div> <div class="periodical"> <span style="font-style: italic;">IEEE International Conference on Robotics and Automation (ICRA)</span>, 2018 <span class="links" style="display: inline-block; margin-left: 10px;"> <a class="bibtex btn btn-sm z-depth-1" role="button">Bib</a> </span> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">amk2018icra</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Voronoi-Based Coverage Control of Pan/Tilt/Zoom Camera Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Arslan, O. and Min, H. and Koditschek, D. E.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Robotics and Automation ({ICRA})}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{5062-5069}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> </ol> <h2>Thesis</h2> <ol class="bibliography"> <li> <div class="row"> <div id="min_PhDThesis2018" class="col-sm-12"> <div class="title">Exploiting Structural Properties in the Analysis of High-dimensional Dynamical Systems</div> <div class="author"> <em>H. Min</em> </div> <div class="periodical"> <span style="font-style: italic;">Ph.D. Thesis, Johns Hopkins University</span>, 2023 <span class="links" style="display: inline-block; margin-left: 10px;"> </span> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div id="min_MScThesis2018" class="col-sm-12"> <div class="title">On Balancing Event and Area Coverage in Mobile Sensor Networks</div> <div class="author"> <em>H. Min</em> </div> <div class="periodical"> <span style="font-style: italic;">Master’s Thesis, University of Pennsylvania</span>, 2018 <span class="links" style="display: inline-block; margin-left: 10px;"> </span> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li> </ol> <h2>Misc</h2> <ol class="bibliography"><li> <div class="row"> <div id="mm2021misc" class="col-sm-12"> <div class="title">Coherence and Concentration in Tightly-Connected Networks</div> <div class="author"> <em>H. Min</em>, <a href="https://www.richardpates.com/" rel="external nofollow noopener" target="_blank">R. Pates</a>, and <a href="https://mallada.ece.jhu.edu/" rel="external nofollow noopener" target="_blank">E. Mallada</a> </div> <div class="periodical"> 2021 <span class="links" style="display: inline-block; margin-left: 10px;"> <a href="http://arxiv.org/abs/2101.00981" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="/assets/pdf/MPM2022TN.pdf" class="btn btn-sm z-depth-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </span> </div> <div class="periodical"> </div> <div class="badges"> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Hancheng Min. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: August 03, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>